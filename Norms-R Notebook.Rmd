---
title: "Creating age-stratified norms in R"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

#### HOW TO USE THIS DOCUMENT:
This R Notebook (.rmd) contains chunks of runnable code along with detailed analysis and documentation of the code and workflow. To use this Notebook, render (knit) it as an HTML document into the RStudio Viewer Pane:

1. In RStudio Preferences, R Markdown section, set the `Show output preview in:` option to `Viewer Pane`.
2. Knit the notebook using `File -> Knit Document`. This will produce a readable HTML version of this document in the Viewer Pane. 
3. Close this Notebook (once you have the HTML in the Viewer, you no longer need the Notebook).
4. Open a new R script in the Source Pane. This will serve as your working script, into which you can copy-and-paste code from the HTML.

The HTML narrative is organized into sections reflecting discrete topics and operations in the R workflow. Each section has two parts:

1. `VALID CODE TO RUN`: this code can be copy-and-pasted into your working R script. Before running, make sure that all file paths relating to input and output files are specified correctly for your local environment. __Note that all code in this Notebook was written for the Mac OS, so if you are working in Windows, you'll need to substitute `\` for `/` in file paths.__
2. `COMMENTED CODE CHUNKS`: these snippets provide the analysis and documentation of the code and workflow. The R code in these snippets is redundant with that in `VALID CODE TO RUN` section. Here, it serves merely to enhance the code documenatation, and is not meant to be run.

<br>

#### UTILIZING RStudio PROJECTS AND DIRECTORY PATHS:
To keep your work organized, set up a project in RStudio, by clicking File -> New Project. You'll be asked to specify a working directory associated with the project. To facilitate running the code of this project easily, locate input files in a folder named `INPUT FILES`, within the working directory associated with this project. Also, create a folder named `OUTPUT FILES`, within the working directory associated with this project. As mentioned above, you need to make sure file paths in your script are specified correctly, in order to read input files and/or write output files.

<br>

#### ESSENTIAL PACKAGES

Packages are the add-on features and functions that give R its power and versatility. Refer to the script `Essential_Packages.R` for the latest list of packages to install in Rstudio, and load at the beginning of every script using `library()`.

To use the code in this notebook, install and run these packages:

###### VALID CODE TO RUN
```{r packages, message=FALSE}
suppressMessages(library(here)) # BEST WAY TO SPECIFY FILE PATHS
library(broom) # TIDY MODEL OUTPUTS
library(moderndive) # USER-FRIENDLY LINEAR MODELING, REGRESSION AND CORRELATION TOOLS.
library(magrittr) # PIPE OPERATORS
# note use of `suppressWarnings` to silence chatter during interactive session
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(library(ggpmisc)) # EXTENSIONS TO ggplot2: ADD EQUATIONS AND FIT STATISTICS TO FITTED LINE PLOTS
library(ggrepel) # MORE ggplot2 EXTENSIONS
```
Note use of `suppressMessages` to silence chatter during startup; and use of `suppressWarnings` to silence chatter during interactive session.

<br>

#### CODE OVERVIEW
This script takes as input a .csv with all cases in the normative sample. Required columns are ID, age group, and score for the variable to be normed. This example uses the total score from the Antonyms test (ANT) of the CASL-2. To run this code, copy this input file to the `INPUT FILES ` folder in your R project working directory: 

`WPS_DEPT\Research & Development\Projects\CASL-2\pd\DSH DOCS\CASL-2 NORMS\ANT\ANTraw_STAND.csv`

<br>

#### Read in data, generate frequency tables by agestrat, descriptive statistics, z-score lookup table

###### VALID CODE TO RUN
```{r freq_desc, eval=FALSE}
ANTraw_by_agestrat <-
  suppressMessages(read_csv(here('INPUT FILES/ANTraw_STAND.csv')
  )) %>% group_by(agestrat)
ANT_tot_freq_agestrat <- ANTraw_by_agestrat %>% count(ANT_total) %>% 
  mutate(perc = round(100*(n/sum(n)), 4), cum_per = round(100*(cumsum(n)/sum(n)), 4), lag_tot = lag(ANT_total), lag_cum_per = lag(cum_per))
ANT_tot_desc_agestrat <-
  ANTraw_by_agestrat %>% arrange(agestrat) %>% summarise(n = n(),
                                                         median = round(median(ANT_total), 2),
                                                         mean = round(mean(ANT_total), 2),
                                                         sd = round(sd(ANT_total), 2)) %>%
  mutate(ES = round((mean - lag(mean))/((sd + lag(sd))/2),2), group = c(1:27))
agestrat <- ANT_tot_desc_agestrat %>% pull(agestrat)
perc_z <- tribble(
  ~lohi_value,	~z_score,
  5, 1.6449,
  10,	1.2816,
  15,	1.0364,
  20,	0.8416,
  25,	0.6745,
  75,	0.6745,
  80,	0.8416,
  85,	1.0364,
  90,	1.2816,
  95,	1.6449
)
```

###### COMMENTED CODE CHUNKS
Read in normative data with total scores for ANT. `here` locates files relative to project root folder. `suppressMessages` keeps console output clean. Data frame is piped into `dplyr::group_by` to group data by agestrat.
```{r freq_desc, echo=1:3, eval=FALSE}
```
Generate frequency table `ANT_tot_freq_agestrat`. Pipe grouped normative data `ANTraw_by_agestrat` into `count(ANT_total)`, which yields two columns: one with all values of `ANT_total`, one `n` with number of cases per each value. Pipe these columns into `mutate`, which generates four more columns: `perc` = percentage of total cases for each value of `ANT_totalt`, `cum_per` = cumulative percentage over increasing values of `ANT_total`, and `lag_tot` and `lag_cum_per`, two 'lag' columns that provide values for these variables from previous row. These columns are rounded `round` to four decimal places.
```{r freq_desc, echo=4:6, eval=FALSE}
```
Generate table of descriptive statistics `ANT_tot_desc_agestrat`. Pipe `ANTraw_by_agestrat` into `arrange`, to sort by agestrat, then into `summarise`, yielding a single-row table showing `mean`, `median`, `sd`, and `ES`, all rounded to two decimal places.
```{r freq_desc, echo=7:11, eval=FALSE}
```
Extract unique values of `agestrat` into vector to be used in other procedures. `pull` extracts a column of a data frame as a vector.
```{r freq_desc, echo=12, eval=FALSE}
```
Use `tribble` to generate z-score lookup table, associating z-scores with normalized percentile points. `tribble` allows you to create a tibble inline, laying out the data in a readable columnar format, with column heads.
```{r freq_desc, echo=13:26, eval=FALSE}
```


<br>


#### Plot raw score means

###### VALID CODE TO RUN
```{r RSmean_plot, eval=FALSE}
mean_plot <- ggplot(data = ANT_tot_desc_agestrat, aes(group, mean)) +
  geom_point(
    col = "blue",
    fill = "blue",
    alpha = .5,
    size = 3,
    shape = 23
  ) +
  scale_x_continuous(breaks = seq(1, 27, 1), labels = agestrat) +
  scale_y_continuous(breaks = seq(0, 55, 5), limits = c(0, 55)) +
  labs(title = "Raw Score Means (with SDs)", x = "Agestrat", y = "Mean Total Score") +
  geom_errorbar(
    aes(ymin = mean - sd, ymax = mean + sd),
    col = "blue",
    size = 0.2,
    width = 0.2
  )
print(mean_plot)
```

###### COMMENTED CODE CHUNKS
`ggplot` takes data frame `ANT_tot_desc_agestrat` as input, yielding a scatterplot `geom_point` that maps agestrat to the x-axis and raw score mean to the y-axis `aes(group, mean)`. Plot output is assigned to `mean_plot` so that plot will print when code is executed from source. `col` controls color of plot point perimeter; `fill` controls color of point interior. Adjust the values of these arguments to change the appearance of the graph. Other useful argments include `alpha` (controls transparency of data element, values 0-1), `size` (of data points), `shape` (indicated by number, look up on internet to see charts).
```{r RSmean_plot, echo=1:8, eval=FALSE}
```
`scale_x_continuous` and `scale_y_continuous` apply new scales to the axes. `breaks = seq(1, 27, 1)` indicates that the axis will be segmented by units of 1, running from 1 to 27. `labels = agestrat` replaces the numerical sequence on the axis with labels from the char vector `agestrat` (note: this vector must have length of 27 for this code to work). `limits = c(0, 55)` sets lower and upper numerical limits on the axis. `labs` specifies the title and super-ordinate labels for the axes.
```{r RSmean_plot, echo=9:11, eval=FALSE}
```
`geom_errorbar` allows you to layer error bars onto a plot. `aes()` is called to map separate variables onto the error bar below the data point `ymin` and the error bar above the data point `ymax`. In this case, SD bars are created by mapping `mean - sd` and `mean + sd`. In `geom_errorbar`, `size` controls the width of the tick at the top/bottom of the bar, and `width` controls the line thickness.
```{r RSmean_plot, echo=12:17, eval=FALSE}
```
Explicit `print` call ensures that plot will print when code is executed from source.
```{r RSmean_plot, echo=18, eval=FALSE}
```

<br>

#### Pause execution for user to examine plot

When the code is sourced, the following chunk pauses execution and prompts the user to examine the newly created plot of raw score means and SDs.

###### VALID CODE TO RUN
```{r mean_plot_prompt, eval=FALSE}
mean_plot_prompt <- function() {
  writeLines(c(
    strrep("\u2500", 40),
    "Examine plot of raw score means and SDs.",
    "Then press [enter] to continue.",
    strrep("\u2500", 40)
  ))
  readline()
}
mean_plot_prompt ()
```
###### COMMENTED CODE CHUNKS
`mean_plot_prompt()` is a function that takes no arguments and thus can be called with empty parentheses.
```{r mean_plot_prompt, echo=1, eval=FALSE}
```
`writeLines` takes a character vector and prints it to the console, where each vector element is printed on a new line. `strrep("\u2500", 40)` repeats a unicode horizontal box drawing character `u2400` 40 times, thus enclosing the printed text between horizontal lines.
```{r mean_plot_prompt, echo=2:7, eval=FALSE}
```
`readline` pauses execution and waits for the user to press [enter], at which point execution resumes.
```{r mean_plot_prompt, echo=8, eval=FALSE}
```

<br>

#### Generate histograms by agestrat

###### VALID CODE TO RUN
```{r hist, eval=FALSE}
hist_plot <- ggplot(data = ANTraw_by_agestrat, aes(ANT_total)) +
  geom_histogram(
    binwidth = .2,
    col = "red"
  ) +
  scale_y_continuous(breaks = seq(0, 20, 1)) +
  labs(title = "Frequency Distribution") + 
  theme(panel.grid.minor=element_blank()) +
  facet_wrap(~agestrat)
print(hist_plot)
```
###### COMMENTED CODE CHUNKS
`ggplot` takes data frame `ANTraw_by_agestrat` as input, yielding histogram `geom_histogram` showing the distribution of variable `aes(ANT_total)`. Plot output is assigned to `hist_plot` so that plot will print when code is executed from source. `binwidth` controls width of histogram columns, `col` controls color of column outlines. Adjust the values of these arguments to change the appearance of the graph. 
```{r hist, echo=1:5, eval=FALSE}
```
`scale_y_continuous` applies new scale to y axis, using argument `breaks`. `seq` sets scale ranging from 0 to 20, in increments of 1. 
```{r hist, echo=6, eval=FALSE}
```
`labs` can be used to provide text labels for the graph, in this case a title.
```{r hist, echo=7, eval=FALSE}
```
`theme` changes non-data elements of graphic, in this case by removing minor gridlines. `element_blank` means 'draw nothing'.
```{r hist, echo=8, eval=FALSE}
```
`facet_wrap` generates separate histogram for each `agestrat`. 
```{r hist, echo=9, eval=FALSE}
```
Explicit `print` call ensures that plot will print when code is executed from source.
```{r hist, echo=10, eval=FALSE}
```
<br>

#### Generate graphs of means, SDs by agestrat.

###### VALID CODE TO RUN
```{r mean_SD_graph, eval=FALSE}
mean_plot <- ggplot(data = ANT_tot_desc_agestrat, aes(group, mean)) +
  geom_point(
    col = "blue",
    fill = "blue", 
    alpha = .5,
    size = 3,
    shape = 23
  ) +
  scale_x_continuous(breaks = seq(1, 27, 1), labels = agestrat) +
  scale_y_continuous(breaks = seq(0, 50, 5), limits = c(0, 50)) + 
  labs(title = "Means", x = "Agestrat", y = "Mean Total Score")
print(mean_plot)
  
sd_plot <- ggplot(data = ANT_tot_desc_agestrat, aes(group, sd)) +
  geom_point(
    col = "red",
    fill = "red", 
    alpha = .5,
    size = 3,
    shape = 23
  ) +
  scale_x_continuous(breaks = seq(1, 27, 1), labels = agestrat) +
  scale_y_continuous(breaks = seq(0, 10, 1), limits = c(0, 10)) + 
  labs(title = "SDs", x = "Agestrat", y = "SD Total Score")
print(sd_plot)
```

<br>

#### Generate table of lo1, lo2, hi1, hi2 cumulative percentage points by agestrat

This section of code takes as input the age-stratified frequency table of the raw score for which norms are being created. The assumptions are that the distribution of raw scores within each age group is skewed (non-normal), and that because of small sample sizes, the age-stratified means and SDs may vary considerably from one age group to the next, and thus may not fit the hypothesized developmental growth curve for the ability being measured. The overall purpose of this R script is to use the input data to model the underlying developmental curve, and then construct well-functioning norms based on that model. 'Well-functioning' in this context means no unexpected changes or reversals in the standard score corresponding to a particular raw score, as the individual being assessed ages from one normative age-group to the next.

The first step of this process is to normalize the raw score distributions within each age group. We begin by selecting five cumulative percentage (percentile) points: the median (`med`), and two points below below the median (`lo1`, `lo2`) and two above (`hi1`, `hi2`), with the latter four being as far out toward the tails of the distribution as possible. The possible values for the `lo` points are 5, 10, 15, 20, and 25, and the possible values for the `hi` points are 75, 80, 85, 90 and 95. 

Ideally, the values for `lo1`, `lo2`, `hi1` and `hi2` would be 5, 10, 90 and 95, respectively. But in order to use 5 as the value for `lo1`, there needs to exist a cumulative percentage in the frequency table that is less than 5. The reason for this is that the formula for imputing normalized raw scores uses values from two adjacent rows in the frequency table. Thus, the value for `lo1` cannot sit on the lowest row of the frequency table, because it needs a trailing row (lag row) in order for the imputation formula to run.

In many small sample size frequency tables, there exists no cumulative frequency value that is less than 5. For example the lowest value in the table might be 7.9, or 11.2. In these cases, the R code sets `lo1` at 10, or 15. The code automatically adjusts the values of `lo1`, `lo2` and `hi1` to ensure that the rows associated with these values always have a trailing row, and so that `lo2` sits on a different row than `lo1`, and `hi1` sits on a different row than `hi2`. In constrast to the other three points, `hi2` is a constant; its value is always set at 95. Because `hi2` sits at the top of the frequency distribution, it always has a trailing row and thus its value can be locked at the optimal value of 95.

`lo1`, `lo2`, `hi1` and `hi2` are used later used to impute raw scores that __WOULD__ be associated with each point, __IF__ the raw scores were normally distributed. The next section of code processes the age-stratified frequency table and chooses lo1, lo2, hi1, and hi2 for each age group.

###### VALID CODE TO RUN
```{r lo_hi_SD, eval=FALSE}
ANT_tot_age_lo1lo2_hi1hi2 <-
  full_join(
    ANT_tot_freq_agestrat,
    (
      ANT_tot_freq_agestrat %>%
        group_by(agestrat) %>%
        summarise(min = min(cum_per)) %>%
        mutate(lo1 = case_when(
          min < 5 ~ 5,
          min < 10 ~ 10,
          min < 15 ~ 15,
          min < 20 ~ 20,
          TRUE ~ 25
        ))
    ),
    by = 'agestrat'
  ) %>%
  group_by(agestrat) %>% mutate(flag = case_when(cum_per > lo1 &
                                                   cum_per < 95 ~ 1,
                                                 TRUE ~ 0)) %>% filter(flag == 1) %>% summarise(min = min(cum_per),
                                                                                                max = max(cum_per),
                                                                                                lo1 = first(lo1)) %>%
  mutate(
    lo2 = case_when(
      lo1 == 5 & min < 10 ~ 10,
      lo1 == 5 & min < 15 ~ 15,
      lo1 == 5 & min < 20 ~ 20,
      lo1 == 5 & min >= 20 ~ 25,
      lo1 == 10 & min < 15 ~ 15,
      lo1 == 10 & min < 20 ~ 20,
      lo1 == 10 & min >= 20 ~ 25,
      lo1 == 15 & min < 20 ~ 20,
      lo1 == 15 & min >= 20 ~ 25,
      TRUE ~ 25
    ),
    hi1 = case_when(max > 90 ~ 90,
                    max > 85 ~ 85,
                    max > 80 ~ 80,
                    TRUE ~ 75),
    hi2 = 95
  ) %>% select(-min,-max)
```

###### COMMENTED CODE CHUNKS
Initializes an output table `ANT_tot_age_lo1lo2_hi1hi2`. `full_join` is analogous to SPSS MATCH FILES, in that it combines the columns of two tables (adding variables, not cases) according to the values of an index variable `by`. `full_join` keeps all values and all rows of the two tables that it combines. The code chunk below identifies the LHS table used by `full_join` as `ANT_tot_freq_agestrat`, the age-stratified frequency table.
```{r lo_hi_SD, echo=1:3, eval=FALSE}
```
The next code chunk identifies the RHS table used by `full_join` as a transformed version of `ANT_tot_freq_agestrat`, the age-stratified frequency table. `ANT_tot_freq_agestrat` is piped `%>%` through a series of functions. First, the table is grouped by agestrat `group_by`. Then it is transformed into a summary table with one row per agestrat, and one variable `min`, which is the minimum value of cumulative percentage `cum_per` within each agestrat. Here `summarise` operates as a quasi-lookup function, finding the minimum value within a numerical vector. `mutate` adds a second column `lo1` to the summary table. `lo1` can take one of five values: 5, 10, 15, 20, or 25; it will take the lowest of these values that is still higher than the  minimum cumulative percentile for that agestrat.  

Thus, values of `lo1` are coded according to values of `min` (the former depends on the latter). `case_when` performs this conditional transformation in a more elegant, readable way than nested `ifelse` statements. `min < 5 ~ 5` states that when `min` is less than 5, `lo1` is coded 5, and so on. `TRUE` is equivalent to ELSE in this conditional logic.  

The last line of this code chunk completes the specification of `full_join`, stating that the LHS and RHS tables are joined `by` the index variable `agestrat`. The data object at this point has the same number of rows as the full frequency table `ANT_tot_freq_agestrat`. `full_join` applies (recycles) the values of the two new variables, `min` and `lo1`, of which there is one value per agestrat, to all cells within a given agestrat in the larger (taller, more rows) table, thus filling out the table completely with those values.
```{r lo_hi_SD, echo=4:17, eval=FALSE}
```
Next we pipe the current data object (a tall table with the same number of rows as `ANT_tot_freq_agestrat`) through a series of transformations that allow it to become the basis for calculating `lo2`. The table is regrouped by `agestrat`, to provide a basis for later collapsing it back into a table with one row per agestrat. `mutate` then adds a new variable `flag` that is coded conditionally by `case_when`, such that cases are flagged if their value on `cum_per` is greater than `lo1` and less than 95. This coding on flag allows the data object to be filtered in a way that simplifies the calculation of `lo2` in a later procedure.  

The object is piped through `filter`, which keeps only the flagged rows. `summarise` transforms the object again into a collapsed table with one row per agestrat, adding three variables: minimum value of `cum_per` per agestrat, maximum value of `cum_per` per agestrat, and first value of `lo1`. Because all values of `lo1` are identical within each agestrat, `first` is a convenient way of simply designating that value for `lo1` in the collapsed table.
```{r lo_hi_SD, echo=18:22, eval=FALSE}
```
The current data object is a table with 27 rows, one for each agestrat. The next code chunk uses `mutate` to code `lo2` (possible values: 10, 15, 20, 25), `hi1` (possible values: 75, 80, 85, 90), and `hi2` (static value: 95). The conditional coding depends on varibles (`min`, `max`) that summarise the distribution of `cum_per` within each agestrat. Thus, R doesn't need to access the distributions in the original age-stratified frequency table `ANT_tot_freq_agestrat` to determine the correct values for `lo2` and `hi1`.
```{r lo_hi_SD, echo=23:40, eval=FALSE}
```
The line of code below cleans up the current object `ANT_tot_age_lo1lo2_hi1hi2` for use in the next procedure. `select` drops the named columns preceded by `-` (minus sign). Without the minus sign, `select` would have kept only the named columns.
```{r lo_hi_SD, echo=41, eval=FALSE}
```

<br>

#### Generate imputed medians, lo and hi SDs by agestrat

The process of normalizing each age-group raw score distribution continues, by adding columns for the imputed median, and standard deviations above and below the median (lo_SD, hi_SD). Dividing the standard deviation into two components contributes to normalization by compensating for the skewness in the original raw score distributions.

###### VALID CODE TO RUN
```{r med_SD_ES, eval=FALSE}
norm_build1 <-
  ANT_tot_freq_agestrat %>% left_join(ANT_tot_age_lo1lo2_hi1hi2, by = 'agestrat') %>% mutate (
    IRS_lo1 = ((lo1 - lag_cum_per) / perc) * (ANT_total - lag_tot) + lag_tot,
    IRS_lo2 = ((lo2 - lag_cum_per) /
                 perc) * (ANT_total - lag_tot) + lag_tot,
    IRS_med = ((50 - lag_cum_per) /
                 perc) * (ANT_total - lag_tot) + lag_tot,
    IRS_hi1 = ((hi1 - lag_cum_per) /
                 perc) * (ANT_total - lag_tot) + lag_tot,
    IRS_hi2 = ((hi2 - lag_cum_per) /
                 perc) * (ANT_total - lag_tot) + lag_tot,
    dist_point = case_when(
      lo1 <= cum_per & lo1 > lag_cum_per ~ 'lo1',
      lo2 <= cum_per &
        lo2 > lag_cum_per ~ 'lo2',
      50 <= cum_per &
        50 > lag_cum_per ~ 'med',
      hi1 <= cum_per &
        hi1 > lag_cum_per ~ 'hi1',
      hi2 <= cum_per &
        hi2 > lag_cum_per ~ 'hi2',
      TRUE ~ NA_character_
    )
  ) %>% drop_na() %>% mutate(IRS = case_when(dist_point == 'lo1' ~ IRS_lo1,
                                                             dist_point == 'lo2' ~ IRS_lo2,
                                                             dist_point == 'med' ~ IRS_med,
                                                             dist_point == 'hi1' ~ IRS_hi1,
                                                             dist_point == 'hi2' ~ IRS_hi2,
                                                             TRUE ~ NA_real_),
                                             RSD = case_when(dist_point == 'lo1' ~ lead(lead(IRS))-IRS,
                                                             dist_point == 'lo2' ~ lead(IRS)-IRS,
                                                             dist_point == 'med' ~ IRS,
                                                             dist_point == 'hi1' ~ IRS-lag(IRS),
                                                             dist_point == 'hi2' ~ IRS-lag(lag(IRS)),
                                                             TRUE ~ NA_real_),
                                             lohi_value = case_when(dist_point == 'lo1' ~ lo1,
                                                             dist_point == 'lo2' ~ lo2,
                                                             dist_point == 'hi1' ~ hi1,
                                                             dist_point == 'hi2' ~ hi2,
                                                             TRUE ~ 50)) %>% 
  select(agestrat, lo1, lo2, hi1, hi2, dist_point, lohi_value, IRS, RSD) %>% left_join(perc_z, by = 'lohi_value') %>% 
  mutate(std_RSD = RSD/z_score, SD = case_when(dist_point == 'lo1' | dist_point == 'hi1' ~ (std_RSD+lead(std_RSD))/2,
                                                                                            TRUE ~ NA_real_),
         median = case_when(dist_point == 'med' ~ IRS,
                            TRUE ~ NA_real_),
         lo_SD = case_when(dist_point == 'lo1' ~ SD,
                           TRUE ~ NA_real_),
         hi_SD = case_when(dist_point == 'hi1' ~ SD,
                           TRUE ~ NA_real_))
    
norm_build_med_hilo_sum <-
  norm_build1 %>% summarise(
    lo1 = first(lo1),
    lo2 = first(lo2),
    hi1 = first(hi1),
    hi2 = first(hi2),
    median = first(na.omit(median)),
    lo_SD = first(na.omit(lo_SD)),
    hi_SD = first(na.omit(hi_SD))
  ) %>% mutate(ES = (median - lag(median))/((hi_SD+lag(hi_SD)+lo_SD+lag(lo_SD))/4), group = 1:27) %>% select(group, everything())
```

###### COMMENTED CODE CHUNKS
Initializes an output table `norm_build1`. Input table is the age-stratified raw score frequency table `ANT_tot_freq_agestrat`. `left_join` matches the input table to the `lo1`, `lo2`, `hi1` and `hi2` normalizing cumulative percentage points for each `agestrat`. `mutate` computes new variables.
```{r med_SD_ES, echo=1:2, eval=FALSE}
```
The first set of new variables created by `mutuate` are the imputed raw scores `IRS_` for the median `med`, `lo1`, `lo2`, `hi1` and `hi2`. The `IRS_` formula includes the raw score `ANT_total` associated with `lo1`, `lo2`, `hi1` or `hi2`, along with the percentage of cases `perc` in that same row. The formula also incorporates the raw score `lag_tot` and cumulative percentage `lag_cum_tot` from the _trailing_ row. `mutate` calculates these values for every row, by `agestrat` even though eventually only the value in the row of `lo1` (for example) will flow through to the next set of calculations.
```{r med_SD_ES, echo=3:11, eval=FALSE}
```
Next `mutate` creates `dist_point` column. This column locates the rows in the table containing values of `cum_per` corresponding to the chosen values for `med`, `lo1`, `lo2`, `hi1` and `hi2`, and applies a label to flag these rows. In the flagged rows, the `lo1` label (for example) identifies the row where the value of `lo1` is less than or equal to the value of `cum_per` in that row, and greater than the value of `cum_per` in the trailing row. `case_when` is used to specify the logical conditions that identify the rows where the labels in `dist_point` are to be applied. The reason for flagging specific rows in this context is that we want to extract specific cells from columns (e.g., the `IRS_` columns) where a value is calculated for every row. We want to filter and pass only the rows corresponding to the `dist_point` labels to later operations.

Note the use of `TRUE ~ NA_character_` as the final argument of `case_when`. `TRUE` specifies the _ELSE_ condition in the `case_when` logic. When using `case_when` for conditional transformations, all RHS must evaluate to the same type of vector. In this instance, we want `TRUE` to evaluate to missing (`NA`), but this is a logical type, whereas the other RHS in this `case_when` are all character. `NA_character_` supports missing values in a character vector.
```{r med_SD_ES, echo=12:23, eval=FALSE}
```
Now the filtering operation: `drop_na()` drops any row that has `NA` in any column. In this instance, it drops all non-flagged rows in `dist_point`, resulting in a pared down table that includes, for each `agestrat`, only the five rows labeled `med`, `lo1`, `lo2`, `hi1` and `hi2`.

`mutate` creates new columns to be conditionally coded by `case_when`. `IRS` collapses the five `IRS_` columns into a single imputed raw score column, by plucking, for each row, the value in the `IRS_` column corresponding to that row's `dist_point`. For example, `case_when` sets the value of `IRS` to `IRS_lo1` when `dist_point` equals `lo1`, thus ignoring the other four `IRS_` columns. 

`RSD` calculates the difference between the imputed raw score for the `lo` and `hi` distribution points, and the imputed raw score for the `med` distibution point. It does this by operating on the `IRS` column, using `lag` and `lead` operators to perform arithmetic on values from trailing and leading rows, and nesting these operators when the target value is two rows away from the current row. 

`lohi_value` associates a numerical percentile value with each `dist_point` label, by collapsing the `lo1`, `lo2`, `hi1` and `hi2` columns into a single column, plucking the target values in a similar manner to the `IRS` calculation described above. In this instance, the `TRUE` argument of `case_when` is used to set the `med` percentile to 50.
```{r med_SD_ES, echo=24:40, eval=FALSE}
```
In the next snippet, `select` subsets columns into only the ones needed going forward. `left_join` looks up normalized z-scores `z_score` associated with the values of `lo1`, `lo2`, `hi1` and `hi2`, using the key variable `by = 'lohi_value'`.

`mutate` creates new columns. `std_RSD` standardizes `RSD` by dividing it by `z_score`.

`SD` estimates the standard deviation separately, above and below the median. It does this by averaging the standardized raw-score difference from the median `std_RSD`, across `lo1` and `lo2`, and separately across `hi1` and `hi2`. The calculation formula uses the `lead` operator to average adjacent cellls in the `std_RSD` column, and uses `case_when` to place the calculated values in the cells associated with `lo1` and `hi1`, setting all other cells in the `SD` column to `NA` (using `TRUE ~ NA_real_`).

`mutate` then creates the columns needed for the final summary table. `median` plucks the imputed median `IRS` associated with `dist_point` `med`, `lo_SD` plucks `SD` associated with `dist_point` `lo1` (the estimated SD _below_ the median), and `hi_SD` plucks `SD` associated with `dist_point` `hi1` (the estimated SD _above_ the median). The values are placed in the same row as the `dist_point` in all three columns, with all other cells set to `NA`.
```{r med_SD_ES, echo=41:49, eval=FALSE}
```
The final snippet creates the summary table `norm_build_med_hilo_sum` used in the next stage of the norming process. `summarise` takes the current data object `norm_build1` (already grouped by `agestrat`) as its input, outputting a summary table with 27 rows (one for each `agestrat`). The new summary columns are all built on columns from `norm_build1` that have only a single value per `agestrat`, meaning that the required value for the summary column can be plucked using the `first` operator.

The challenge with `median`, `lo_SD`, and `hi_SD` is that the single required value is embedded among multiple `NA`. To prevent `first` from plucking `NA`, it is wrapped around `na.omit`, which causes `first` to ignore `NA` until it finds a numerical value.

Finally, mutate adds an effect size column `ES` to the summary table, which gives the effect size of the difference between medians from one `agestrat` to the next. It then adds an ordinal `group` column and moves that to left-most position with `select` and the helper function `everything()`, i.e., specifying the column position of `group` relative to all other columns in the table.
```{r med_SD_ES, echo=51:60, eval=FALSE}
```

<br>

#### Use linear modeling to estimate "smoothed" medians, lo_SDs, and hi_SDs.

The imputed means and SDs retain the fluctuating variability between age-groups that was present in the original raw-score distributions. The next step is to smooth out this variability by fitting regression lines to the scatter plots for age vs. med, age vs. lo_SD, and age vs. hi_SD, and using the regression coefficients from these lines to generate "smoothed" medians, lo_SDs and hi_SDs for each age group. These smoothed values become the basis for the norms moving forward.

To generate the smoothed median for each age group, you choose between a 1st, 2nd, or 3rd-order polynomial equation as the smoothing forumula. The code generates fitted plots for each of these models, showing the regression coefficients and R^2^ for each, allowing you to choose the appropriate model. The 3rd-order polynomial usually provides the best fit. You can also inspect the plots for the 2nd- and 1st-order polynomials, to determine if a reasonably good fit can be achieved with a more parsimonius model. 

For the lo_SDs and hi SDs, the code produces plots of the 1st-order (straight line) model and the mean substitution (horizontal line) model. Again, you select the model by examining the plots and judging which model best fits the data.

Model selection is done via an interactive session. The console lists the model choices and prompts you to examine the plots and make a selection, successively for median, lo_SD, and hi_SD. In order for this interactive feature to work, the entire script must be executed from source, using `command-shift-S`.

###### VALID CODE TO RUN
```{r plot_model_med, eval=FALSE}
rm(ANT_tot_age_lo1lo2_hi1hi2, ANTraw_by_agestrat, norm_build1)

frm <- c('y ~ x', 'y ~ x + I(x^2)', 'y ~ x + I(x^2) + I(x^3)')
title <-
  c('1st order polynomial',
    '2nd order polynomial',
    '3rd order polynomial')
lo_hi <- c('lo_SD', 'hi_SD')

med_plots <- map2(
  title,
  frm,
  ~
    ggplot(norm_build_med_hilo_sum, aes(x = group, y = median)) +
    geom_point(
      col = 'red',
      fill = 'red',
      alpha = .5,
      size = 3,
      shape = 23
    ) +
    labs(
      x = 'age group'
      ,
      y = 'median'
      ,
      title = paste0('Median: ', .x)
    ) +
    scale_x_continuous(breaks = seq(1, 27, 1), labels = agestrat) +
    scale_y_continuous(breaks = seq(0, 50, 5)) +
    geom_smooth(
      method = 'lm',
      se = FALSE,
      formula = .y
    ) +
    stat_poly_eq(
      formula = .y,
      eq.with.lhs = 'italic(hat(y))~`=`~',
      aes(label = paste(..eq.label.., ..rr.label.., sep = '*plain(\',\')~')),
      parse = TRUE
    )
)
print(med_plots)

model_median_1st <- function() {
  lm(median ~ group, data = norm_build_med_hilo_sum) %>% assign('model_median', ., envir = .GlobalEnv)
  get_regression_points(model_median, digits = 6) %>% pull(median_hat) %>% assign('median_sm', ., envir = .GlobalEnv)
}

model_median_2nd <- function() {
  lm(median ~ group + I(group ^ 2), data = norm_build_med_hilo_sum) %>% assign('model_median', ., envir = .GlobalEnv)
  get_regression_points(model_median, digits = 6) %>% pull(median_hat) %>% assign('median_sm', ., envir = .GlobalEnv)
}

model_median_3rd <- function() {
  lm(median ~ group + I(group ^ 2) + I(group ^ 3), data = norm_build_med_hilo_sum) %>% assign('model_median', ., envir = .GlobalEnv)
  get_regression_points(model_median, digits = 6) %>% pull(median_hat) %>% assign('median_sm', ., envir = .GlobalEnv)
}

model_prompt_median <- function() {
  writeLines(
    "Examine plots and choose smoothing model for medians:\n\n1: First-order polynomial: y = ax + b\n2: Second-order polynomial: y= ax^2 + bx + c\n3: Third-order polynomial: y= ax^3 + bx^2 + cx + d\n"
  )
  model_choice <- as.numeric(0)
  while (is.na(model_choice) || (!(model_choice %in% 1:3))) {
    model_choice <-
      suppressWarnings(as.numeric(readline(prompt = "Enter choice: ")))
    if (is.na(model_choice)) {
      writeLines("Please enter 1, 2, or 3\n")
    } else {
      if (model_choice == 1) {
        model_median_1st()
        break
      } else if (model_choice == 2) {
        model_median_2nd()
        break
      } else if (model_choice == 3) {
        model_median_3rd()
        break
      } else {
        writeLines("Please enter 1, 2, or 3\n")
      }
    }
  }
}
model_prompt_median()
```

<br>

#### Plot imputed medians with fitted regression lines

###### COMMENTED CODE CHUNKS
Clean up environment by dropping superfluous tables.
```{r plot_model_med, echo=1, eval=FALSE}
```
Define char vectors containing formulas, plot labels. Note the formula format used for the 2nd- and 3rd-order polynomials in `frm`. The 2nd- and 3rd-order terms `x^2` and `x^3` are wrapped in `I()`. This forces R to evaluate `^` as as an _arithmetic_ operator (instead of as a _formula_ operator), and thus the polynomial expression is evaluated correctly. 

As an aside, R has another format for higher-order polynomial formulas `poly()`, (e.g., `poly(x, 3)` to express a 3rd-order polynomial). `poly()` doesn't work in the current application, because it produces an orthoganol polynomial solution, which doesn't lead to usable predicted values when used to model the smoothed medians.
```{r plot_model_med, echo=3:8, eval=FALSE}
```
Use `purrr::map2` to iterate over two parallel vectors of equal length. In this case, `map2` renders three scatterplots of the imputed median, by iterating over `title` (containing plot titles) and `frm` (containing regression line formulas), substituting elements of those vectors as arguments in a `ggplot` call. Note that this sets up _parallel_ iteration, where corresponding elements are drawn from both input vectors at each iteration, as opposed to _nested_ iteration, which iterates through all elements of a subordinate vector at (for) each element of a superordinate vector. 

In the `map2` call, the first two arguments are the input vectors `title` and `frm`. These are followed by `~`, a shorthand for designating an anonymous function. The iteration tokens `.x` and `.y` appear in the body of the function at the places where `map2` substitutes elements of the input vectors, with `.x` functioning as a placeholder for elements of the first input vector `title`, and `.y` functioning as a placeholder for the elements of the second input vector `frm`.

Note that the plots rendered by `map2` are assigned to the object `med_plots`, so that the plots will print when the code is sourced.
```{r plot_model_med, echo=10:13, eval=FALSE}
```
`ggplot` takes as input the table of imputed medians, lo_SD, hi_SD `norm_build_med_hilo_sum`. It generates a scatterplot `geom_point` of age group on the x-axis and median on the y-axis `aes(x = group, y = median)`. (Note: some lines/arguments of the `ggplot` call were documented earlier in this Notebook, and will not be repeated here.)
```{r plot_model_med, echo=14:15, eval=FALSE}
```
Substitution for `.x` from the `title` vector takes place in the following line of code, which uses `paste0` to concatenate string elements to create a `title` for each of the three plots.
```{r plot_model_med, echo=27, eval=FALSE}
```
`ggplot2` allows the creation of layered plots. In the current instance, `geom_point()` created the base layer of a scatterplot, `labs` added a layer of labels, and now `geom_smooth()` will be used to layer on a regression line. (`+` is used to add an additional layer). `method` specifies the smoothing or modeling approach (in this case `lm` for linear model). `se = FALSE` indicates that no error bars will be shown on the regression line. `formula = .y` designates the regression equation to be used; here `.y` is a placeholder for the polynomial formulas contained in `frm`, which will be substituted as `map2` iterates.
```{r plot_model_med, echo=31:35, eval=FALSE}
```
The final layer is specified by `ggpmisc::stat_poly_eq` which prints the corresponding regression equation and fit statistic on each plot, yielding three complete plots that can be used to compare the fits of 1st-, 2nd- and 3rd-order polynomial models. `map2` subtitutes the model formulas for `.y` in `formula = .y`. `eq.with.lhs` designates the string that prints to form the LHS of the regression equation. `aes` creates a `label` that prints to serve as the RHS of the regression equation: `paste` creates the `label` string by concatenating two string elements: the equation for the fitted polynomial `..eq.label..`, and the R^2^ for the fitted polynomial `..rr.label..` (`sep` designates the sequence of characters used to separate these two elements within the parsed string). `parse = TRUE` converts the label text into an expression (which R can perform actions on).
```{r plot_model_med, echo=36:41, eval=FALSE}
```
Explicit `print` command ensures that plots will print when code is executed from source.
```{r plot_model_med, echo=43, eval=FALSE}
```

<br>

#### Generate smoothed (estimated) medians for three linear models

As noted in the previous section, the norming process continues with the choice of models to smooth out the inter-age group variability in the imputed medians. In the smoothing process, `group` is used to predict (estimate) the smoothed values for the imputed medians. The regression coefficients from the selected linear model are used to generate the smoothed values for each age group. `group` is used as a linear, equal-interval proxy for the ordinal sequence of `agestrat` labels. Somewhat counterintuitively, the imputed values of median are _not_ used to estimate the smoothed values of those same variables. Using `group` as the independent variable in the linear models provides the desired smoothed values.

###### COMMENTED CODE CHUNKS
The script contains three named functions (`model_median_1st`, `model_median_2nd`, `model_median_3rd`) that return vectors holding estimated (smoothed) medians for the 1st-, 2nd-, and 3rd-order polynomial formulas. Because the three functions are analogous, only the 3rd-order function is documented in detail.

`model_median_3rd` is a function whose body contains two operations. The first operation is the estimation of the linear model using `lm`. The first argument of `lm` specifies the terms of the 3rd-order polynomial formula, in which `group` is used to predict `median`. The second argument of `lm`, `data =`, specifies that the input values for model estimation are taken from the summary table `norm_build_med_hilo_sum` created by upstream code. The output of `lm` is piped into `assign`, whose first argument specifies a named object `model_median` to hold the model ouput. The second argument of `assign`,  `.`, specifies that the piped input will be assigned to `model_median`, and the third argument `envir = .GlobalEnv` specifies that `model_median` will be available in the global environment, where it can be processed in downstream functions.

The second operation of `model_median_3rd` creates a numerical vector `median_sm` that holds the smoothed (predicted) median values across the levels of `agestrat`. `moderndive::get_regression_points` extracts the regression coefficients from `model_median` and uses them to generate the smooted medians. The output of `get_regression_points` is a tibble in which one column `median_hat` contains the smoothed medians. This output is piped through `dplyr::pull` which extracts the medians column, which is assigned to `median_sm`.

```{r plot_model_med, echo=55:58, eval=FALSE}
```

<br>

#### Use interactive session to select median smoothing model

Previous code plotted imputed medians vs. agestrats, and fit 1st-, 2nd-, and 3rd-order polynomial regression lines to the imputed medians. This code also created functions that generate the estimated (smoothed) medians associated with each of these models. 

When sourced, the script generates the plots and prompts you to examine the plots and enter a choice for smoothing model at the console. The next set of snippets encodes a function that lists the model choices on the console and prompts you to select a model by entering its number. The smoothed medians associated with the selected model are thereby incorporated in all downstream code.

This interactive session can only be executed from source by entering `command-shift-S`. The code cannot be run as an isolated block using `command-enter`; this mode of execution won't pause and wait for user input.

###### COMMENTED CODE CHUNKS
The function that enables this interactive session is designated `model_prompt_median`. `writeLines` prints user instructions for model selection to the console as a numbered list, using the escape character `\n` to insert line breaks in the printed text.
```{r plot_model_med, echo=60:63, eval=FALSE}
```
`model_choice` is initialized as a numeric vector with placeholder value of 0. Within the function `model_prompt_median`, `model_choice` will hold the numerical value that the user enters when prompted by the console instructions. The value of `model_choice` controls the flow of logic within `model_prompt_median`; its value determines which of the three smoothed median estimation functions will be executed. Note that the assignment operator `<-` is used to initialize `model_choice`, rather than the `assign` function that was used in the bodies of the smoothed median estimation functions. The reason for this is that `model_choice` is a temporary variable that is not utilized outside of the body of this particular function. Thus, it does not need to be made available in the global environment. When `<-` is used within the body of a function, the object thereby created is avaiable _only_ in the local environment of that function. In this instance, we can get away with using `<-` because `model_choice` is not needed in the global environment.
```{r plot_model_med, echo=64, eval=FALSE}
```
The selected smoothed median estimation function is called and executed within a `while` loop, which iterates as long as a compound logical condition is met (logical "or" `||`). This condition is specified by `is.na(model_choice) || (!(model_choice %in% 1:3)`, which tells the `while` loop to keep iterating as long as the value of `model_choice` is anything other than 1, 2, or 3 (i.e., the user enters a non-permitted value at the console). Thus, if the user enters an integer other than 1, 2, or 3, or any other keystroke(s), including `enter` without an entry, the loop will continue to iterate and control will _not_ be passed to downstream code. `is.na(model_choice)` is included in this compound condition because any keystroke other than 1, 2, or 3 is coerced to `NA` within this function. Thus, the loop keeps iterating if the function returns `NA`.
```{r plot_model_med, echo=65, eval=FALSE}
```
Within the `while` loop, `model_choice` is respecified to read user input from the console. `readline` allows the specification of a prompt at the console, and reads user input, which is then assigned to `model_choice`. Wrapping `readline` in `suppressWarnings` keeps the printed text on the console clean and readable.
```{r plot_model_med, echo=66:67, eval=FALSE}
```
`if`-`else` statements control the flow of operations within the loop. The first `if` statement specifies that if `model_choice` is `NA` (i.e., the user enters a non-permitted keystroke(s) at the console), `writeLines` prints a reminder to enter a permitted keystroke, and then control returns to the beginning of the loop, where `readlines` again prompts the user to enter a model choice.
```{r plot_model_med, echo=68:69, eval=FALSE}
```
Next comes a nested logical flow that is subsumed under `else`. When control enters the nested flow, the logical condition is that `model_choice` is _not_ `NA`. The nested `if`-`else` statements control what happens when the user enters a permitted choice (i.e., the value of `model_choice` is 1, 2, or 3). Under each of these three conditions (specified by sequential `if`-`else if`-`else if` commands), the smoothed median function associated with a particular value of `model_choice` is called, followed by `break` which causes control to exit the loop and continue into downstream code. Thus, the `model_median_prompt` function prompts the user to select a model at the console, repeats the prompt when a non-permitted choice is entered, and ultimately executes one of three smoothed median estimation functions and makes the output of that function available to downstream code.
```{r plot_model_med, echo=70:79, eval=FALSE}
```
A final `else` condition is triggered by any user input not subsumed under the prior logical conditions. It reprompts the user, and returns control to the beginning of the loop. As noted above, the loop will only stop running if the user enters 1, 2, or 3.
```{r plot_model_med, echo=80:81, eval=FALSE}
```
`model_prompt_median` is called with empty parentheses `()` because it takes no arguments. The function simply executes the code within its body, it does not depend on any input values specified in the function call.
```{r plot_model_med, echo=86, eval=FALSE}
```

<br>

#### Estimate smoothed lo_SDs, hi_SDs

The code for estimating smoothed values for the lo_SDs and hi_SDs is analogous to that detailed above for the smoothed medians. The operations flow from plotting the imputed values with lines for 1st-order polynomial and mean-subsitution smoothing models, to initializing functions that will smooth the SDs based on these two models, to using an interactive session to select the smoothing models for the SDs. This process is completed first for the lo_SDs, and then for the hi_SDs. This section of code concludes by generating the summary table that is used in downstream norming processes, and removing unneeded objects from the global environment.

The code is only documented in detail where it differs from previous code.

###### VALID CODE TO RUN
```{r plot_model_SDs, eval=FALSE}
lo_SD_1st_plot <-
  ggplot(norm_build_med_hilo_sum, aes(x = group, y = lo_SD)) +
  geom_point(
    col = 'red',
    fill = 'red',
    alpha = .5,
    size = 3,
    shape = 23
  ) +
  labs(x = 'age group'
       ,
       y = "lo_SD"
       ,
       title = 'lo_SD 1st order polynomial') +
  scale_x_continuous(breaks = seq(1, 27, 1), labels = agestrat) +
  scale_y_continuous(breaks = seq(0, 10, 1)) +
  geom_smooth(method = 'lm',
              se = FALSE,
              formula = y ~ x) +
  stat_poly_eq(
    formula = y ~ x,
    eq.with.lhs = 'italic(hat(y))~`=`~',
    aes(label = paste(..eq.label.., ..rr.label.., sep = '*plain(\',\')~')),
    parse = TRUE
  )

mean_lo_SD <- round(mean(norm_build_med_hilo_sum$lo_SD), 2)
lo_SD_mean_plot <-
  ggplot(norm_build_med_hilo_sum, aes(x = group, y = lo_SD)) +
  geom_point(
    col = 'red',
    fill = 'red',
    alpha = .5,
    size = 3,
    shape = 23
  ) +
  labs(x = 'age group'
       ,
       y = 'lo_SD'
       ,
       title = 'lo_SD mean substitution') +
  scale_x_continuous(breaks = seq(1, 27, 1), labels = agestrat) +
  scale_y_continuous(breaks = seq(0, 10, 1)) +
  geom_hline(yintercept = mean_lo_SD) +
  geom_label(aes(5, mean_lo_SD, label = paste0("Mean lo_SD = ", mean_lo_SD)))

print(lo_SD_1st_plot)
print(lo_SD_mean_plot)

model_lo_SD_1st <- function() {
  lm(lo_SD ~ group, data = norm_build_med_hilo_sum) %>% assign('model_lo_SD', ., envir = .GlobalEnv)
  get_regression_points(model_lo_SD, digits = 6) %>% pull(lo_SD_hat) %>% assign('lo_SD_sm', ., envir = .GlobalEnv)
}

model_lo_SD_mean <- function() {
  norm_build_med_hilo_sum %>% mutate(mean_lo_SD = mean_lo_SD) %>% pull(mean_lo_SD) %>% assign('lo_SD_sm', ., envir = .GlobalEnv)
  enframe(mean(norm_build_med_hilo_sum$lo_SD), name = NULL, value = "model_lo_SD") %>% assign('model_lo_SD', ., envir = .GlobalEnv)
}

model_prompt_lo_SD <- function() {
  writeLines(
    "\nExamine plots and choose smoothing model for lo_SDs:\n\n1: First-order polynomial: y = ax + b\n2: Mean substitution: y= mean(x)\n"
  )
  model_choice <- as.numeric(0)
  while (is.na(model_choice) || (!(model_choice %in% 1:2))) {
    model_choice <-
      suppressWarnings(as.numeric(readline(prompt = "Enter choice: ")))
    if (is.na(model_choice)) {
      writeLines("Please enter 1 or 2\n")
    } else {
      if (model_choice == 1) {
        model_lo_SD_1st()
        break
      } else if (model_choice == 2) {
        model_lo_SD_mean()
        break
      } else {
        writeLines("Please enter 1 or 2\n")
      }
    }
  }
}
model_prompt_lo_SD()

hi_SD_1st_plot <-
  ggplot(norm_build_med_hilo_sum, aes(x = group, y = hi_SD)) +
  geom_point(
    col = 'red',
    fill = 'red',
    alpha = .5,
    size = 3,
    shape = 23
  ) +
  labs(x = 'age group'
       ,
       y = "hi_SD"
       ,
       title = 'hi_SD 1st order polynomial') +
  scale_x_continuous(breaks = seq(1, 27, 1), labels = agestrat) +
  scale_y_continuous(breaks = seq(0, 10, 1)) +
  geom_smooth(method = 'lm',
              se = FALSE,
              formula = y ~ x) +
  stat_poly_eq(
    formula = y ~ x,
    eq.with.lhs = 'italic(hat(y))~`=`~',
    aes(label = paste(..eq.label.., ..rr.label.., sep = '*plain(\',\')~')),
    parse = TRUE
  )

mean_hi_SD <- round(mean(norm_build_med_hilo_sum$hi_SD), 2)
hi_SD_mean_plot <-
  ggplot(norm_build_med_hilo_sum, aes(x = group, y = hi_SD)) +
  geom_point(
    col = 'red',
    fill = 'red',
    alpha = .5,
    size = 3,
    shape = 23
  ) +
  labs(x = 'age group'
       ,
       y = 'hi_SD'
       ,
       title = 'hi_SD mean substitution') +
  scale_x_continuous(breaks = seq(1, 27, 1), labels = agestrat) +
  scale_y_continuous(breaks = seq(0, 10, 1)) +
  geom_hline(yintercept = mean_hi_SD) +
  geom_label(aes(5, mean_hi_SD, label = paste0("Mean hi_SD = ", mean_hi_SD)))

print(hi_SD_1st_plot)
print(hi_SD_mean_plot)

model_hi_SD_1st <- function() {
  lm(hi_SD ~ group, data = norm_build_med_hilo_sum) %>% assign('model_hi_SD', ., envir = .GlobalEnv)
  get_regression_points(model_hi_SD, digits = 6) %>% pull(hi_SD_hat) %>% assign('hi_SD_sm', ., envir = .GlobalEnv)
}

model_hi_SD_mean <- function() {
  norm_build_med_hilo_sum %>% mutate(mean_hi_SD = mean_hi_SD) %>% pull(mean_hi_SD) %>% assign('hi_SD_sm', ., envir = .GlobalEnv)
  enframe(mean(norm_build_med_hilo_sum$hi_SD), name = NULL, value = "model_hi_SD") %>% assign('model_hi_SD', ., envir = .GlobalEnv)
}

model_prompt_hi_SD <- function() {
  writeLines(
    "\nExamine plots and choose smoothing model for hi_SDs:\n\n1: First-order polynomial: y = ax + b\n2: Mean substitution: y= mean(x)\n"
  )
  model_choice <- as.numeric(0)
  while (is.na(model_choice) || (!(model_choice %in% 1:2))) {
    model_choice <-
      suppressWarnings(as.numeric(readline(prompt = "Enter choice: ")))
    if (is.na(model_choice)) {
      writeLines("Please enter 1 or 2\n")
    } else {
      if (model_choice == 1) {
        model_hi_SD_1st()
        break
      } else if (model_choice == 2) {
        model_hi_SD_mean()
        break
      } else {
        writeLines("Please enter 1 or 2\n")
      }
    }
  }
}
model_prompt_hi_SD()

smooth_med_SD <-
  cbind(norm_build_med_hilo_sum, median_sm, lo_SD_sm, hi_SD_sm) %>% dplyr::select(-(lo1:hi2)) %>%
  mutate(ES_sm = round((median_sm - lag(median_sm)) / ((
    lo_SD_sm + lag(lo_SD_sm) + hi_SD_sm + lag(hi_SD_sm)
  ) / 4), 3))

rm(list = ls()[!ls() %in% c("smooth_med_SD", "model_median", "model_lo_SD", "model_hi_SD")])
```







###### COMMENTED CODE CHUNKS

The mean-subsitution model smooths the SDs by substituting their mean for every imputed value. The lo_SD mean is generated by the following code (taking the mean of the column of imputed lo_SD values, specified by `norm_build_med_hilo_sum$lo_SD`). The value is assigned to `mean_lo_SD`, a numerical vector of length 1.
```{r plot_model_SDs, echo=27, eval=FALSE}
```

The next two lines are part of the `ggplot` call to add a horizontal line to the mean-substitution plot. That line is layered on by `geom_hline`, which draws a horizontal line at the value of `mean_lo_SD` using the `yintercept` argument. `geom_label` applies a label to the line with the value of the mean (the first argument of `aes`, 5, refers to the position of the label relative to the line).
```{r plot_model_SDs, echo=44:45, eval=FALSE}
```

The next snippet addresses the fact that the output of model choice for lo_SD (and hi_SD) can be either a linear model or a constant (mean of SDs). Downstream code needs either type of output to be in an object named `model_lo_SD`. `enframe` puts the constant into a 1x1 tibble, `name = NULL` gets rid of an unneeded column, and `value` controls the name of the one remaining column. Downstream, a logical gate controls the code applied to `model_lo_SD`, depending on whether that object contains the linear model or the tibble.
```{r plot_model_SDs, echo=55:59, eval=FALSE}
```

The next snippet creates the output table `smooth_med_SD` used in the next stage of the norming process. `base::cbind` joins columns of equal length into a data frame, in this case joining the input table `norm_build_med_hilo_sum` to the new columns of smoothed values: `median_sm`, `lo_SD_sm`, and `hi_SD_sm`. This combined table is piped through `dplyr::select` to remove unneeded columns, and then through `dplyr::mutate`, to create a colmn of effect sizes for the smoothed medians. In the effect size formula, note the use of `lag` to bring in values from the preceeding row.

```{r plot_model_SDs, echo=169:173, eval=FALSE}
```


